{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXu1CGD1ftRX"
   },
   "source": [
    "#### Homework 1\n",
    "\n",
    "1. build the MLP in the image above using PT built-ins\n",
    "2. Provide calculation for the exact number of parameters of the MLP\n",
    "   - Do it first supposing that the layers don't have a bias term, then supposing that the bias is present wherever it's possible\n",
    "3. Calculate the $L_1$ (vectorial) norm and the Frobenius norm for the params of each layer\n",
    "4. Given 10 random datapoints, feed them into the network. This operation must be done all in one single command and must **not** make use of loops.\n",
    "   - Given the output of the network, using PyTorch code, find the class of assignment of each datapoint. This also must be done in a single PyTorch command without using loops.\n",
    "   - Drafting a vector of ground truths (whichever labels you like), provide code for the calculation of the accuracy\n",
    "     - Tip: first get the number of correct assignments, then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS9HWPIgfsg5",
    "outputId": "1cc66c08-4ae4-463a-be05-075e3f7f3c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SEV2eNPoH3X"
   },
   "source": [
    "## 1. Build the MLP in the image above using PT built-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7iKtPsMdf-zy"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.layers = torch.nn.Sequential(\n",
    "           torch.nn.Linear(5, 11),\n",
    "           torch.nn.ReLU(),\n",
    "           torch.nn.Linear(11, 16),\n",
    "           torch.nn.ReLU(), \n",
    "           torch.nn.Linear(16, 13),\n",
    "           torch.nn.ReLU(),\n",
    "           torch.nn.Linear(13, 8),\n",
    "           torch.nn.ReLU(),\n",
    "           torch.nn.Linear(8, 4), \n",
    "           torch.nn.Softmax(dim=1)\n",
    "      )\n",
    "        \n",
    "    def forward(self, X):\n",
    "      return self.layers(X)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kuSQiF5hoZc"
   },
   "source": [
    "## 2. Provide calculation for the exact number of parameters of the MLP\n",
    "\n",
    "Supposing that no bias term is present, we have that the number of weights is given by the sum across the layers of the multiplication of input and output dimension of a nueron in the layer (since the architecture is dense), without taking into accont the input nuerons.\n",
    "\n",
    "$$ \\sum_l in_l \\times out_l = 5 \\times 11 + 11 \\times 16 + 16 × 13 + 13 \\times 8 + 8 \\times 4 = 575$$\n",
    "\n",
    "If we consider the bias term we have to add a number of weight corresponding to the number of neurons that are not input nodes: \n",
    "$$ \\sum_{l!=0} n_i = 11+16+13+8+4 = 52$$\n",
    "\n",
    "The total number of weight will therefore be: \n",
    "$$\\sum_l in_l \\times out_l + \\sum_{l!=0} n_i = 575 +52 = 627$$\n",
    "\n",
    "In the cells below the summary of the model is presented and confirms that the calculation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--L4TTzWkep4",
    "outputId": "381fa632-5a23-464e-d0a7-91c7e53abfd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       66\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       192\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       221\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Linear: 2-7                       112\n",
       "│    └─ReLU: 2-8                         --\n",
       "│    └─Linear: 2-9                       36\n",
       "│    └─Softmax: 2-10                     --\n",
       "=================================================================\n",
       "Total params: 627\n",
       "Trainable params: 627\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yvnBNU3oP1O"
   },
   "source": [
    "## 3.Calculate the L1 (vectorial) norm and the Frobenius norm for the params of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhBmJO4Noh_l",
    "outputId": "a53bbf6d-893d-4b6b-96d6-d80862176f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm:  [12.455259323120117, 25.966724395751953, 26.8754940032959, 14.715418815612793, 6.213144779205322]\n",
      "Frobenious norm:  [1.911424160003662, 2.27777099609375, 2.1251652240753174, 1.6577861309051514, 1.2323379516601562]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_norm(norm):\n",
    "  return [l.weight.norm(p=norm).numpy().item() for l in model.layers\n",
    "          if isinstance(l, torch.nn.Linear)]\n",
    "\n",
    "print(\"L1 norm: \", get_norm(1))\n",
    "print(\"Frobenious norm: \", get_norm('fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goDzOWtOuA8Q"
   },
   "source": [
    "## 4. Given 10 random datapoints, feed them into the network. This operation must be done all in one single command and must not make use of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmkfIHiY-nl2",
    "outputId": "21cb23fb-f2d6-411b-b1e2-b261258dac34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: tensor([2, 3, 3, 0, 3, 1, 1, 0, 0, 2])\n",
      "out: tensor([[0.2323, 0.1684, 0.2854, 0.3139],\n",
      "        [0.2320, 0.1708, 0.2842, 0.3130],\n",
      "        [0.2318, 0.1709, 0.2842, 0.3131],\n",
      "        [0.2318, 0.1703, 0.2845, 0.3134],\n",
      "        [0.2319, 0.1701, 0.2845, 0.3135],\n",
      "        [0.2316, 0.1712, 0.2840, 0.3131],\n",
      "        [0.2325, 0.1682, 0.2856, 0.3137],\n",
      "        [0.2316, 0.1704, 0.2845, 0.3136],\n",
      "        [0.2317, 0.1697, 0.2848, 0.3137],\n",
      "        [0.2320, 0.1703, 0.2844, 0.3133]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.randint(0, 4, (10,))\n",
    "print(\"true:\", y_true)\n",
    "y_out = model(torch.rand((10, 5)))\n",
    "print(\"out:\", y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lN6zwAVO_3iC"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "  return torch.mean((y_true == y_pred).float())\n",
    "\n",
    "def get_pred_from_out(y_out):\n",
    "  return torch.max(y_out, dim=1).indices\n",
    "\n",
    "def accuracy_from_output(y_true, y_out):\n",
    "  return accuracy(y_true, get_pred_from_out(y_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-KzUq8wCfv4",
    "outputId": "fe82ef07-b9c5-40cc-ea99-9afe0f66d409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: tensor([2, 3, 3, 0, 3, 1, 1, 0, 0, 2])\n",
      "pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "acc : 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "print(\"true:\", y_true)\n",
    "print(\"pred:\", get_pred_from_out(y_out))\n",
    "print(\"acc :\", accuracy_from_output(y_true, y_out).numpy().item())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DL_Assignmen1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
